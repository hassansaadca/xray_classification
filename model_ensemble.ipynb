{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "\n",
    "import xray_data #y data: 1 = NORMAL, 0 = PNEUMONIA\n",
    "import random\n",
    "\n",
    "random.seed(207)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step: Load and preprocess data\n",
    "Note: data is resized and preprocessed as read, as a memory optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "scale = 200\n",
    "subset = 'PROP'\n",
    "label_filter = ['NORMAL','PNEUMONIA','COVID19','TURBERCULOSIS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (10 of 10) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (8 of 8) |##########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (8 of 8) |##########################| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "100% (12 of 12) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID19: 8\n",
      "PNEUMONIA: 8\n",
      "NORMAL: 8\n",
      "TURBERCULOSIS: 8\n",
      "Total: 32\n",
      "X_dev_orig, y_dev_orig shape: ((32, 40000), (32,))\n",
      "y_dev_orig shape for NORMAL cases: (8,)\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "X_dev_orig, y_dev_orig = xray_data.load_val(scale,label_filter)\n",
    "print(f'X_dev_orig, y_dev_orig shape: {X_dev_orig.shape, y_dev_orig.shape}')\n",
    "print(f'y_dev_orig shape for NORMAL cases: {y_dev_orig[y_dev_orig ==1].shape}')\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (106 of 106) |######################| Elapsed Time: 0:00:10 Time:  0:00:10\n",
      "100% (390 of 390) |######################| Elapsed Time: 0:00:05 Time:  0:00:05\n",
      "100% (234 of 234) |######################| Elapsed Time: 0:00:08 Time:  0:00:08\n",
      "100% (41 of 41) |########################| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID19: 106\n",
      "PNEUMONIA: 390\n",
      "NORMAL: 234\n",
      "TURBERCULOSIS: 41\n",
      "Total: 771\n",
      "X_test, y_test shape: ((771, 40000), (771,))\n",
      "y_test shape for NORMAL cases: (390,)\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# cut of for training samples of each class, only 230 normal rows\n",
    "\n",
    "X_test, y_test = xray_data.load_test(scale,label_filter,subset = subset)\n",
    "print(f'X_test, y_test shape: {X_test.shape, y_test.shape}')\n",
    "print(f'y_test shape for NORMAL cases: {y_test[y_test ==1].shape}')\n",
    "print('----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (460 of 460) |######################| Elapsed Time: 0:00:37 Time:  0:00:37\n",
      "100% (3875 of 3875) |####################| Elapsed Time: 0:01:13 Time:  0:01:13\n",
      "100% (1341 of 1341) |####################| Elapsed Time: 0:00:56 Time:  0:00:56\n",
      "100% (650 of 650) |######################| Elapsed Time: 0:00:10 Time:  0:00:10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID19: 460\n",
      "PNEUMONIA: 3875\n",
      "NORMAL: 1341\n",
      "TURBERCULOSIS: 650\n",
      "Total: 6326\n"
     ]
    }
   ],
   "source": [
    "# cut of for training samples of each class, only 1300 normal rows\n",
    "\n",
    "X_train, y_train = xray_data.load_train(scale,label_filter,subset = subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train, y_train shape: ((5060, 40000), (5060,))\n",
      "y_train shape for NORMAL cases: (3099,)\n",
      "----\n",
      "X_dev, y_dev shape: ((1266, 40000), (1266,))\n",
      "y_dev shape for NORMAL cases: (776,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size = .2, stratify = y_train, random_state = 207 )\n",
    "print(f'X_train, y_train shape: {X_train.shape, y_train.shape}')\n",
    "print(f'y_train shape for NORMAL cases: {y_train[y_train ==1].shape}')\n",
    "print('----')\n",
    "print(f'X_dev, y_dev shape: {X_dev.shape, y_dev.shape}')\n",
    "print(f'y_dev shape for NORMAL cases: {y_dev[y_dev ==1].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Ensembles\n",
    "\n",
    "First, using a hard voting structure where we just take the mode of the classifications from n models.\n",
    "\n",
    "Second, using a soft voting structure in which the probabilities are weighed evenly. One issue with this is that the logistic regression model performs best when the data is scaled. On the flip side, the Naive Bayes model can't take in scaled data (i.e. negative values). So, we try with unscaled data to start. \n",
    "\n",
    "We simulate the creation of another ensemble model below for which we first scale the data passed into the logistic regression model, then find the optimal weight distribution across all models which maximizes the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***HARD VOTING BASELINE ENSEMBLE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hassansaad/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#create an ensemble model that takes the mode of the classifications of the underlying single models\n",
    "clf1 = KNeighborsClassifier(n_neighbors = 3, metric = 'euclidean')\n",
    "clf2 = svm.SVC(C = 10,gamma = .0001,kernel='rbf', probability = True)\n",
    "clf3= LogisticRegression(max_iter = 1000)\n",
    "clf4 = MultinomialNB(alpha = 71)\n",
    "\n",
    "\n",
    "ensemble_hard = VotingClassifier(estimators=[('KNN', clf1),('SVM',clf2),('LR', clf3),('MNB',clf4)], voting='hard')\n",
    "ensemble_hard = ensemble_hard.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9454976303317536"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print hard voting mechanism score\n",
    "\n",
    "ensemble_hard.score(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***SOFT VOTING BASELINE ENSEMBLE***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hassansaad/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#create an ensemble model that averages the predict_proba_ values for each individual values...\n",
    "#...then makes a classification based off the average probability\n",
    "\n",
    "ensemble_soft = VotingClassifier(estimators=[('KNN', clf1),('SVM',clf2),('LR', clf3),('MNB',clf4)], \n",
    "                             voting='soft'\n",
    "                            )\n",
    "ensemble_soft = ensemble_soft.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9447077409162717"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print soft voting mechanism score\n",
    "ensemble_soft.score(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ensemble_soft.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "grid = confusion_matrix(y_dev, pred)\n",
    "plt.imshow(grid, cmap = 'summer')\n",
    "\n",
    "for (j,i),label in np.ndenumerate(grid):\n",
    "    plt.text(i,j,label,ha='center',va='center')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.xticks([0,1,2,3])\n",
    "plt.yticks([0,1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = ensemble_soft._collect_probas(X_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Average of Prediction Probabilities\n",
    "\n",
    "Since we want to feed in a scaled version of the data for the logistic regression model to function best (and we can't feed standard scaled data to the multinomial NB model), we manually build an ensemble model using a soft voting system. \n",
    "\n",
    "We iterate (inefficiently) through various weights to see which yeilds the best performance for the ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = KNeighborsClassifier(n_neighbors = 3, metric = 'euclidean')\n",
    "clf2 = svm.SVC(C = 10,gamma = .0001,kernel='rbf', probability = True)\n",
    "clf3= LogisticRegression(max_iter = 1000)\n",
    "clf4 = MultinomialNB(alpha = 71)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hassansaad/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_sc = sc.transform(X_train)\n",
    "X_dev_sc = sc.transform(X_dev)\n",
    "\n",
    "clf1.fit(X_train, y_train)\n",
    "clf2.fit(X_train, y_train)\n",
    "clf3.fit(X_train_sc, y_train)\n",
    "clf4.fit(X_train, y_train)\n",
    "\n",
    "prob1 = clf1.predict_proba(X_dev) #knn\n",
    "prob2 = clf2.predict_proba(X_dev) #svm\n",
    "prob3 = clf3.predict_proba(X_dev_sc) #log\n",
    "prob4 = clf4.predict_proba(X_dev) #NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob1_weights = np.arange(0,31) #knn\n",
    "prob2_weights = np.arange(0,31) #svm\n",
    "prob3_weights = np.arange(0,31) #log\n",
    "prob4_weights = np.arange(0,31) #NB\n",
    "weights = []\n",
    "scores = []\n",
    "\n",
    "\n",
    "for i in prob1_weights:\n",
    "    for j in prob2_weights:\n",
    "        for k in prob3_weights:\n",
    "            for l in prob4_weights:\n",
    "                probs = (prob1*i+prob2*j+prob3*k+prob4*l)/4\n",
    "                pred= probs.argmax(axis = 1)\n",
    "                score = (pred == y_dev).sum()/pred.shape[0]\n",
    "                weights.append([i,j,k,l])\n",
    "                scores.append(score)\n",
    "                \n",
    "weights = np.array(weights)\n",
    "scores = np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_weights = (weights[scores.argmax()]/weights[scores.argmax()].sum())*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_score = (scores[scores.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest Ensemble Score: 0.9605055292259084.\n",
      "Optimal Weights: [ 7.69230769 57.69230769 30.76923077  3.84615385].\n"
     ]
    }
   ],
   "source": [
    "print(f'Highest Ensemble Score: {highest_score}.')\n",
    "print(f'Optimal Weights: {optimal_weights}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-2e4b9f5a8294>:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  pd.DataFrame(weights/weights.sum(axis=1, keepdims=True)).fillna(0)], axis =1)\n"
     ]
    }
   ],
   "source": [
    "#create a dataframe of the weights applied to each individual model within the ensemble as well as the percentage\n",
    "weight_df = pd.concat([pd.DataFrame(weights),\n",
    "           pd.DataFrame(weights/weights.sum(axis=1, keepdims=True)).fillna(0)], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_df.columns = ['knn','svm','log','naive','knn_weight','svm_weight','log_weight','naive_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>knn</th>\n",
       "      <th>svm</th>\n",
       "      <th>log</th>\n",
       "      <th>naive</th>\n",
       "      <th>knn_weight</th>\n",
       "      <th>svm_weight</th>\n",
       "      <th>log_weight</th>\n",
       "      <th>naive_weight</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.211690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.751185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.936809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.770142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.754344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923503</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>0.291262</td>\n",
       "      <td>0.291262</td>\n",
       "      <td>0.291262</td>\n",
       "      <td>0.126214</td>\n",
       "      <td>0.953397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923507</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>0.280374</td>\n",
       "      <td>0.280374</td>\n",
       "      <td>0.280374</td>\n",
       "      <td>0.158879</td>\n",
       "      <td>0.950237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923509</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>0.275229</td>\n",
       "      <td>0.275229</td>\n",
       "      <td>0.275229</td>\n",
       "      <td>0.174312</td>\n",
       "      <td>0.949447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923513</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>0.265487</td>\n",
       "      <td>0.265487</td>\n",
       "      <td>0.265487</td>\n",
       "      <td>0.203540</td>\n",
       "      <td>0.947867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923519</th>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>0.252101</td>\n",
       "      <td>0.252101</td>\n",
       "      <td>0.252101</td>\n",
       "      <td>0.243697</td>\n",
       "      <td>0.946288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>841842 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        knn  svm  log  naive  knn_weight  svm_weight  log_weight  \\\n",
       "0         0    0    0      0    0.000000    0.000000    0.000000   \n",
       "1         0    0    0      1    0.000000    0.000000    0.000000   \n",
       "31        0    0    1      0    0.000000    0.000000    1.000000   \n",
       "32        0    0    1      1    0.000000    0.000000    0.500000   \n",
       "33        0    0    1      2    0.000000    0.000000    0.333333   \n",
       "...     ...  ...  ...    ...         ...         ...         ...   \n",
       "923503   30   30   30     13    0.291262    0.291262    0.291262   \n",
       "923507   30   30   30     17    0.280374    0.280374    0.280374   \n",
       "923509   30   30   30     19    0.275229    0.275229    0.275229   \n",
       "923513   30   30   30     23    0.265487    0.265487    0.265487   \n",
       "923519   30   30   30     29    0.252101    0.252101    0.252101   \n",
       "\n",
       "        naive_weight     score  \n",
       "0           0.000000  0.211690  \n",
       "1           1.000000  0.751185  \n",
       "31          0.000000  0.936809  \n",
       "32          0.500000  0.770142  \n",
       "33          0.666667  0.754344  \n",
       "...              ...       ...  \n",
       "923503      0.126214  0.953397  \n",
       "923507      0.158879  0.950237  \n",
       "923509      0.174312  0.949447  \n",
       "923513      0.203540  0.947867  \n",
       "923519      0.243697  0.946288  \n",
       "\n",
       "[841842 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#then drop the duplicate percentages\n",
    "weight_df['score'] = scores\n",
    "weight_df.drop_duplicates(subset = ['knn_weight','svm_weight','log_weight','naive_weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>knn</th>\n",
       "      <th>svm</th>\n",
       "      <th>log</th>\n",
       "      <th>naive</th>\n",
       "      <th>knn_weight</th>\n",
       "      <th>svm_weight</th>\n",
       "      <th>log_weight</th>\n",
       "      <th>naive_weight</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.751185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.936809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29791</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.893365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       knn  svm  log  naive  knn_weight  svm_weight  log_weight  naive_weight  \\\n",
       "1        0    0    0      1         0.0         0.0         0.0           1.0   \n",
       "31       0    0    1      0         0.0         0.0         1.0           0.0   \n",
       "961      0    1    0      0         0.0         1.0         0.0           0.0   \n",
       "29791    1    0    0      0         1.0         0.0         0.0           0.0   \n",
       "\n",
       "          score  \n",
       "1      0.751185  \n",
       "31     0.936809  \n",
       "961    0.955766  \n",
       "29791  0.893365  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_df[(weight_df.knn +weight_df.svm +weight_df.log + weight_df.naive == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging Ensemble\n",
    "\n",
    "Model using parameters from our best guess model from our non-ensemble versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPclf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2), max_iter = 1000, random_state=1)\n",
    "baggingclf = BaggingClassifier(MLPclf, max_samples=0.5, max_features=0.5)\n",
    "\n",
    "baggingclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_pred_dev = baggingclf.predict(X_dev)\n",
    "print(f'Bagging Multi Layer Perceptron Model Accuracy: {f1_score(bagging_pred_dev,y_dev,average=\"weighted\")*100:9.5}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost Ensemble\n",
    "\n",
    "AdaBoost uses low accuracy classifiers, with weighting.  Given the complexity of our feature space we would not expect simple classifiers to be very good predictors, even when combined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB_clf = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "AB_clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB_clf_pred_dev = AB_clf.predict(X_dev)\n",
    "print(f'Adaboost Model Accuracy: {f1_score(AB_clf_pred_dev,y_dev,average=\"weighted\")*100:9.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
